{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pathlib\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "VIDEO_FOLDER_PATH = 'videos'\n",
    "PATH_FOR_IMAGES = 'images'\n",
    "PATH_FOR_IMAGES = pathlib.Path(PATH_FOR_IMAGES)\n",
    "SEQUENCE_LENGTH=20\n",
    "\n",
    "VALIDATION_PERCENTAGE = 0.20\n",
    "TEST_PERCENTAGE = 0.10\n",
    "TRAIN_PERCENTAGE = 1 - (VALIDATION_PERCENTAGE + TEST_PERCENTAGE)\n",
    "\n",
    "\n",
    "CLASSES = np.array(sorted([item.name for item in PATH_FOR_IMAGES.glob('*') if item.is_dir()]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. (Data Preparation) Extract frames from the video and store in new folder location in project directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in os.listdir(VIDEO_FOLDER_PATH):\n",
    "\n",
    "    # TODO: Check if images folder exists, if not create it\n",
    "    if not os.path.exists(PATH_FOR_IMAGES):\n",
    "        os.mkdir(PATH_FOR_IMAGES)\n",
    "    \n",
    "    if not os.path.exists(os.path.join(PATH_FOR_IMAGES, folder)):\n",
    "        os.mkdir(os.path.join(PATH_FOR_IMAGES, folder))\n",
    "\n",
    "    for video_file in os.listdir(os.path.join(VIDEO_FOLDER_PATH, folder)):\n",
    "        full_video_path = os.path.join(VIDEO_FOLDER_PATH, folder, video_file)\n",
    "        video = cv2.VideoCapture(full_video_path)\n",
    "        success, image = video.read()\n",
    "        count = 0\n",
    "\n",
    "        while success:\n",
    "            # video.set(cv2.CAP_PROP_POS_MSEC,(count*200))\n",
    "            image_path = os.path.join(PATH_FOR_IMAGES, folder, \"frame%d.jpg\" % count)\n",
    "            cv2.imwrite(image_path, image)\n",
    "            success, image = video.read()\n",
    "            count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (Data Partioning) Create TensorFlow Datasets Objects consisting of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset dictionary with train, test and validation paritions, where the images are stored in a list\n",
    "def create_dataset(path):\n",
    "    dataset = {}\n",
    "    dataset['train'] = list()\n",
    "    dataset['test'] = list()\n",
    "    dataset['validation'] = list()\n",
    "\n",
    "    for folder in os.listdir(path):\n",
    "        folder_path = os.path.join(path, folder)\n",
    "        # Split the images in the folder into train, test and validation sets\n",
    "        images = [os.path.join(path, folder, item.name) for item in pathlib.Path(folder_path).glob('*') if item.is_file()]\n",
    "        images = np.array(images)\n",
    "        split = int(len(images) * TRAIN_PERCENTAGE)\n",
    "        train_images = images[:split]\n",
    "        test_images = images[split:]\n",
    "        validation_split = int(len(test_images) * VALIDATION_PERCENTAGE)\n",
    "        validation_images = test_images[:validation_split]\n",
    "        test_images = test_images[validation_split:]\n",
    "\n",
    "        dataset['train'].extend(train_images)\n",
    "        dataset['test'].extend(test_images)\n",
    "        dataset['validation'].extend(validation_images)\n",
    "\n",
    "        print(\"Folder: %s, train: %d, test: %d, validation: %d\" % (folder, len(train_images), len(test_images), len(validation_images)))\n",
    "    return dataset\n",
    "            \n",
    "dataset = create_dataset(PATH_FOR_IMAGES)\n",
    "\n",
    "print(\"Total training images: %d\" % len(dataset['train']))\n",
    "print(\"Total test images: %d\" % len(dataset['test']))\n",
    "print(\"Total validation images: %d\" % len(dataset['validation']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_image_count = len(list(PATH_FOR_IMAGES.glob('**/*.jpg')))\n",
    "# # NOTE: Below does not need to be a tensorflow dataset\n",
    "# list_ds = tf.data.Dataset.list_files(str(PATH_FOR_IMAGES/'*/*.jpg'), shuffle=False)\n",
    "# # list_ds = list_ds.shuffle(total_image_count, reshuffle_each_iteration=False)\n",
    "\n",
    "\n",
    "# validation_set_size = int(total_image_count * VALIDATION_PERCENTAGE)\n",
    "# training_set_size = int(total_image_count * TRAIN_PERCENTAGE)\n",
    "# test_set_size = int(total_image_count * TEST_PERCENTAGE)\n",
    "\n",
    "# training_set = list_ds.take(training_set_size)\n",
    "# validation_set = list_ds.skip(training_set_size).take(validation_set_size)\n",
    "# test_set = list_ds.skip(validation_set_size + training_set_size).take(test_set_size)\n",
    "\n",
    "# print(\"Amount of Training data: \" + str(tf.data.experimental.cardinality(training_set).numpy()))\n",
    "# print(\"Amount of Validation data: \" + str(tf.data.experimental.cardinality(validation_set).numpy()))\n",
    "# print(\"Amount of Test data: \" + str(tf.data.experimental.cardinality(test_set).numpy()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. (Data Pre-processing) Detect person, Create image, label pair data points, augment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label_from_path(image_path):\n",
    "    parts = tf.strings.split(image_path, os.path.sep)\n",
    "    one_hot = tf.dtypes.cast(parts[-2] == CLASSES, tf.int16)\n",
    "    # NOTE: Converting back tensor to numpy array, as the tensor is not serializable\n",
    "    return tf.argmax(one_hot).numpy()\n",
    "\n",
    "\n",
    "def decode_image(img):\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    return tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
    "\n",
    "def process_path(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = decode_image(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the image path list and load the images in batches containing 20 images\n",
    "# Each batch is assigned a label based on the predominant class in the batch\n",
    "# Repeat the process for the entire image path list offsetting the iteration by the sequence length\n",
    "def load_dataset(list_of_images):\n",
    "    full_dataset_images = []\n",
    "    full_dataset_labels = []\n",
    "    temp_set = []\n",
    "    temp_labels = []\n",
    "    for image_path in list_of_images:\n",
    "        if len(temp_set) == SEQUENCE_LENGTH:\n",
    "            main_label = max(set(temp_labels), key=temp_labels.count)\n",
    "            full_dataset_images.append(temp_set)\n",
    "            full_dataset_labels.append(main_label)\n",
    "            temp_set = []\n",
    "            temp_labels = []\n",
    "        else:\n",
    "            temp_set.append(process_path(image_path))\n",
    "            temp_labels.append(extract_label_from_path(image_path))\n",
    "    return np.asarray(full_dataset_images), tf.keras.utils.to_categorical(np.asarray(full_dataset_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensorflow dataset object from the dataset dictionary\n",
    "# NOTE: This is a tensorflow dataset object\n",
    "\n",
    "train_ds = load_dataset(dataset['train'])\n",
    "validation_ds = load_dataset(dataset['validation'])\n",
    "test_ds = load_dataset(dataset['test'])\n",
    "\n",
    "# training_set = tf.data.Dataset.from_tensor_slices(dataset['train'])\n",
    "# validation_set = tf.data.Dataset.from_tensor_slices(dataset['validation'])\n",
    "# test_set = tf.data.Dataset.from_tensor_slices(dataset['test'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_ds:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_for_performance(ds):\n",
    "    ds = ds.cache()\n",
    "    ds = ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.batch(batch_size=32)\n",
    "    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = configure_for_performance(train_ds)\n",
    "validation_ds = configure_for_performance(validation_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(train_ds))\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "print(image_batch.shape)\n",
    "\n",
    "for j, images in enumerate(image_batch):\n",
    "    for i in range(9):\n",
    "     ax = plt.subplot(3, 3, i + 1)\n",
    "     plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "     label = label_batch[j][i]\n",
    "     plt.title(CLASSES[label])\n",
    "     plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. (Model Implementation) Neural Network Implementation\n",
    "* We might not need to actually shuffle the dataset as we want to capture temporal relationship\n",
    "* Need to consider the sequence length \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.ConvLSTM2D(filters=16, kernel_size=(3, 3), activation=\"tanh\",  padding='same', recurrent_dropout=0.2, return_sequences=True, input_shape=(SEQUENCE_LENGTH, IMG_HEIGHT, IMG_WIDTH, 3)))\n",
    "model.add(keras.layers.MaxPooling3D(pool_size=(1, 2, 2), padding='same'))\n",
    "model.add(keras.layers.TimeDistributed(keras.layers.Dropout(0.2)))\n",
    "\n",
    "model.add(keras.layers.ConvLSTM2D(filters=32, kernel_size=(3, 3), activation=\"tanh\",  padding='same', recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(keras.layers.MaxPooling3D(pool_size=(1, 2, 2), padding='same'))\n",
    "model.add(keras.layers.TimeDistributed(keras.layers.Dropout(0.2)))\n",
    "\n",
    "model.add(keras.layers.ConvLSTM2D(filters=64, kernel_size=(3, 3), activation=\"tanh\",  padding='same', recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(keras.layers.MaxPooling3D(pool_size=(1, 2, 2), padding='same'))\n",
    "model.add(keras.layers.TimeDistributed(keras.layers.Dropout(0.2)))\n",
    "\n",
    "model.add(keras.layers.ConvLSTM2D(filters=128, kernel_size=(3, 3), activation=\"tanh\",  padding='same', recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(keras.layers.MaxPooling3D(pool_size=(1, 2, 2), padding='same'))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Visualisation with TensorBoard\n",
    "path = log_dir=\"logs\\\\fit\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=path, histogram_freq=1, update_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n",
    "hist = model.fit(x=train_ds[0], y=train_ds[1], epochs=200, batch_size=32, validation_data=validation_ds, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate trained model on test dataset\n",
    "# model.evaluate(test_batches)cey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from video2tfrecord import convert_videos_to_tfrecord\n",
    "video_path = os.path.join(\"videos\", \"move\", \"1.mp4\")\n",
    "print(video_path)\n",
    "video_path_2 = \"C:/Users/LoveShark/Desktop/Projects\\move_detection/videos/move/1.mp4\"\n",
    "convert_videos_to_tfrecord(video_path_2, 'videos/move/1.tfrecord', 1, \"all\", \"*.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_hub as hub\n",
    "import cv2\n",
    "import numpy as np\n",
    "#To make tf 2.0 compatible with tf1.0 code, we disable the tf2.0 functionalities\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "# Loading TF Hub module\n",
    "model_handle = 'https://tfhub.dev/google/tiny_video_net/tvn2/1'\n",
    "model = hub.Module(model_handle)\n",
    "# video = tf.io.read_file('videos/move/1.mp4')\n",
    "# predictions = model(video)\n",
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_video(path, max_frames=0, resize=(256, 256)):\n",
    "  cap = cv2.VideoCapture(path)\n",
    "  frames = []\n",
    "  try:\n",
    "    while True:\n",
    "      ret, frame = cap.read()\n",
    "      if not ret:\n",
    "        break\n",
    "    #   frame = crop_center_square(frame)\n",
    "      frame = cv2.resize(frame, resize)\n",
    "      frame = frame[:, :, [2, 1, 0]]\n",
    "      frames.append(frame)\n",
    "      \n",
    "      if len(frames) == max_frames:\n",
    "        break\n",
    "  finally:\n",
    "    cap.release()\n",
    "  return np.array(frames) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sample_video):\n",
    "    # Add a batch axis to the sample video.\n",
    "    model_input = tf.constant(sample_video, dtype=tf.float32)\n",
    "    print(model_input.shape)\n",
    "    return model(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 256, 256, 3)\n",
      "(16, 256, 256, 3)\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'module_2_apply_default/dense/BiasAdd:0' shape=(8, 157) dtype=float32>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_path = os.path.join(\"videos\", \"move\", \"1.mp4\")\n",
    "video_test = load_video(video_path)\n",
    "# Take the first 16 frames to match the model's expected input shape.\n",
    "video_test = video_test[:16]\n",
    "predictions = predict(video_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c000 Holding some clothes',\n",
       " 'c001 Putting clothes somewhere',\n",
       " 'c002 Taking some clothes from somewhere',\n",
       " 'c003 Throwing clothes somewhere',\n",
       " 'c004 Tidying some clothes',\n",
       " 'c005 Washing some clothes',\n",
       " 'c006 Closing a door',\n",
       " 'c007 Fixing a door',\n",
       " 'c008 Opening a door',\n",
       " 'c009 Putting something on a table',\n",
       " 'c010 Sitting on a table',\n",
       " 'c011 Sitting at a table',\n",
       " 'c012 Tidying up a table',\n",
       " 'c013 Washing a table',\n",
       " 'c014 Working at a table',\n",
       " 'c015 Holding a phone/camera',\n",
       " 'c016 Playing with a phone/camera',\n",
       " 'c017 Putting a phone/camera somewhere',\n",
       " 'c018 Taking a phone/camera from somewhere',\n",
       " 'c019 Talking on a phone/camera',\n",
       " 'c020 Holding a bag',\n",
       " 'c021 Opening a bag',\n",
       " 'c022 Putting a bag somewhere',\n",
       " 'c023 Taking a bag from somewhere',\n",
       " 'c024 Throwing a bag somewhere',\n",
       " 'c025 Closing a book',\n",
       " 'c026 Holding a book',\n",
       " 'c027 Opening a book',\n",
       " 'c028 Putting a book somewhere',\n",
       " 'c029 Smiling at a book',\n",
       " 'c030 Taking a book from somewhere',\n",
       " 'c031 Throwing a book somewhere',\n",
       " 'c032 Watching/Reading/Looking at a book',\n",
       " 'c033 Holding a towel/s',\n",
       " 'c034 Putting a towel/s somewhere',\n",
       " 'c035 Taking a towel/s from somewhere',\n",
       " 'c036 Throwing a towel/s somewhere',\n",
       " 'c037 Tidying up a towel/s',\n",
       " 'c038 Washing something with a towel',\n",
       " 'c039 Closing a box',\n",
       " 'c040 Holding a box',\n",
       " 'c041 Opening a box',\n",
       " 'c042 Putting a box somewhere',\n",
       " 'c043 Taking a box from somewhere',\n",
       " 'c044 Taking something from a box',\n",
       " 'c045 Throwing a box somewhere',\n",
       " 'c046 Closing a laptop',\n",
       " 'c047 Holding a laptop',\n",
       " 'c048 Opening a laptop',\n",
       " 'c049 Putting a laptop somewhere',\n",
       " 'c050 Taking a laptop from somewhere',\n",
       " 'c051 Watching a laptop or something on a laptop',\n",
       " 'c052 Working/Playing on a laptop',\n",
       " 'c053 Holding a shoe/shoes',\n",
       " 'c054 Putting shoes somewhere',\n",
       " 'c055 Putting on shoe/shoes',\n",
       " 'c056 Taking shoes from somewhere',\n",
       " 'c057 Taking off some shoes',\n",
       " 'c058 Throwing shoes somewhere',\n",
       " 'c059 Sitting in a chair',\n",
       " 'c060 Standing on a chair',\n",
       " 'c061 Holding some food',\n",
       " 'c062 Putting some food somewhere',\n",
       " 'c063 Taking food from somewhere',\n",
       " 'c064 Throwing food somewhere',\n",
       " 'c065 Eating a sandwich',\n",
       " 'c066 Making a sandwich',\n",
       " 'c067 Holding a sandwich',\n",
       " 'c068 Putting a sandwich somewhere',\n",
       " 'c069 Taking a sandwich from somewhere',\n",
       " 'c070 Holding a blanket',\n",
       " 'c071 Putting a blanket somewhere',\n",
       " 'c072 Snuggling with a blanket',\n",
       " 'c073 Taking a blanket from somewhere',\n",
       " 'c074 Throwing a blanket somewhere',\n",
       " 'c075 Tidying up a blanket/s',\n",
       " 'c076 Holding a pillow',\n",
       " 'c077 Putting a pillow somewhere',\n",
       " 'c078 Snuggling with a pillow',\n",
       " 'c079 Taking a pillow from somewhere',\n",
       " 'c080 Throwing a pillow somewhere',\n",
       " 'c081 Putting something on a shelf',\n",
       " 'c082 Tidying a shelf or something on a shelf',\n",
       " 'c083 Reaching for and grabbing a picture',\n",
       " 'c084 Holding a picture',\n",
       " 'c085 Laughing at a picture',\n",
       " 'c086 Putting a picture somewhere',\n",
       " 'c087 Taking a picture of something',\n",
       " 'c088 Watching/looking at a picture',\n",
       " 'c089 Closing a window',\n",
       " 'c090 Opening a window',\n",
       " 'c091 Washing a window',\n",
       " 'c092 Watching/Looking outside of a window',\n",
       " 'c093 Holding a mirror',\n",
       " 'c094 Smiling in a mirror',\n",
       " 'c095 Washing a mirror',\n",
       " 'c096 Watching something/someone/themselves in a mirror',\n",
       " 'c097 Walking through a doorway',\n",
       " 'c098 Holding a broom',\n",
       " 'c099 Putting a broom somewhere',\n",
       " 'c100 Taking a broom from somewhere',\n",
       " 'c101 Throwing a broom somewhere',\n",
       " 'c102 Tidying up with a broom',\n",
       " 'c103 Fixing a light',\n",
       " 'c104 Turning on a light',\n",
       " 'c105 Turning off a light',\n",
       " 'c106 Drinking from a cup/glass/bottle',\n",
       " 'c107 Holding a cup/glass/bottle of something',\n",
       " 'c108 Pouring something into a cup/glass/bottle',\n",
       " 'c109 Putting a cup/glass/bottle somewhere',\n",
       " 'c110 Taking a cup/glass/bottle from somewhere',\n",
       " 'c111 Washing a cup/glass/bottle',\n",
       " 'c112 Closing a closet/cabinet',\n",
       " 'c113 Opening a closet/cabinet',\n",
       " 'c114 Tidying up a closet/cabinet',\n",
       " 'c115 Someone is holding a paper/notebook',\n",
       " 'c116 Putting their paper/notebook somewhere',\n",
       " 'c117 Taking paper/notebook from somewhere',\n",
       " 'c118 Holding a dish',\n",
       " 'c119 Putting a dish/es somewhere',\n",
       " 'c120 Taking a dish/es from somewhere',\n",
       " 'c121 Wash a dish/dishes',\n",
       " 'c122 Lying on a sofa/couch',\n",
       " 'c123 Sitting on sofa/couch',\n",
       " 'c124 Lying on the floor',\n",
       " 'c125 Sitting on the floor',\n",
       " 'c126 Throwing something on the floor',\n",
       " 'c127 Tidying something on the floor',\n",
       " 'c128 Holding some medicine',\n",
       " 'c129 Taking/consuming some medicine',\n",
       " 'c130 Putting groceries somewhere',\n",
       " 'c131 Laughing at television',\n",
       " 'c132 Watching television',\n",
       " 'c133 Someone is awakening in bed',\n",
       " 'c134 Lying on a bed',\n",
       " 'c135 Sitting in a bed',\n",
       " 'c136 Fixing a vacuum',\n",
       " 'c137 Holding a vacuum',\n",
       " 'c138 Taking a vacuum from somewhere',\n",
       " 'c139 Washing their hands',\n",
       " 'c140 Fixing a doorknob',\n",
       " 'c141 Grasping onto a doorknob',\n",
       " 'c142 Closing a refrigerator',\n",
       " 'c143 Opening a refrigerator',\n",
       " 'c144 Fixing their hair',\n",
       " 'c145 Working on paper/notebook',\n",
       " 'c146 Someone is awakening somewhere',\n",
       " 'c147 Someone is cooking something',\n",
       " 'c148 Someone is dressing',\n",
       " 'c149 Someone is laughing',\n",
       " 'c150 Someone is running somewhere',\n",
       " 'c151 Someone is going from standing to sitting',\n",
       " 'c152 Someone is smiling',\n",
       " 'c153 Someone is sneezing',\n",
       " 'c154 Someone is standing up from somewhere',\n",
       " 'c155 Someone is undressing',\n",
       " 'c156 Someone is eating something']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load charades class labels\n",
    "with open('charades_classes.txt', 'r') as f:\n",
    "    CLASSES = [line.strip() for line in f.readlines()]\n",
    "CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_5:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d3201d0cc1ba9ebba05104869d43dbcadf69309420631ec72c1b51e9998a3d6c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('movedetection': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
